Copyright Â© 2021 by Boston Consulting Group. All rights reserved

# Infrastructure Guide

This document describes all the computing infrastructure and data storage required to run the tool, and how to gain access to them. As well as the different data sources


Table of Contents
=================

* [Infrastructure Requirements](#infrastructure-requirements)
* [List of Input Data Sources](#list-of-input-data-sources)
    * [Incremental Files](#incremental-files)
    * [Static Files](#static-files)
    * [Historical Dumps](#historical-dumps)
* [Outputs Created](#outputs-created)
   * [Outputs generated by Pipeline](#outputs-generated-by-pipeline)
   * [IT File Outputs](#it-file-outputs)

## Infrastructure Requirements

Infrastructure | Description                      | Owner / PoC | Access control
--- |----------------------------------| --- | ---


  
## List of Input Data Sources

File Name Prefix | Description 
--- | --- 




### Static Files 

All static files used to generate the recommendations.

**Root Folder Name:** **`input_data/`**


File Name Prefix | Description 
--- | --- 


## Outputs Created

### Outputs generated by Pipeline

To understand the outputs created by the pipelines, one has to understand the concept of a **Context**.

A **Context** consists of 3 things:
 - **Pipeline**:  pipeline is a series of steps which are performed in sequence to accomplish an objective (e.g. the data engineering pipeline defines steps required to cleans & pre-processes raw input data)
 - **Run and run_id**: A run is define as a specific instance of executing a pipeline at a moment in time. For example, If I "run" the data engineering pipeline 3 times in a row at one minute intervals, I would be executing 3 separate "runs" of the data engineering pipeline, with 3 unique times of execution. There would be a unique **run_id** associated with each run. 
 - **Task**: A Task is a specific step within a pipeline that performs a modular operation. A pipeline is composed of tasks.
 
Using the 3 components of the context, we manage the storage of output data in the directory structure.  :  

Example Data Layout:
```
s3://<bucket-name>/              # S3 Root Path
  output_data/
    {root_folder_name}/          # Name of the root folder specified in `.env` file. E.g. dev or prod or dev_johndoe etc.
      {pipeline_name}/           # Name of the pipeline. E.g. ingest_source_data
        {run_id}/                # Timstamped folder corresponding to the time of execution
          {task_name}/           # Name of the task. E.b. ingest_inventory
            file_output_1.xlsx   # File produced by the run
        latest/                  # Folder corressponding to the latest run
          {task_name}/           #
            file_output_1.xlsx   #                
```

The root_folder_name is provided in the `.env` file to ensure that multiple developers can create output_data in their own folders without hampering the work of others. Please refer to [Installation Guide](INSTALLATION_GUIDE.md) for more details on how to set this parameter

All the files that the respective tasks generate are defined as attributes of the class `AllDataStores` in the data_stores.py

### Example Output Screenshot

![Outputs Screenshot](img/output_data_screenshot.png "Outputs Screenshot")


